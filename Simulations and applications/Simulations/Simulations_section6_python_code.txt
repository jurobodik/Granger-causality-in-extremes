#Simulations from section 6 state-of-the-art methods performance. This code deals with the Tigramite paclage
#R code can be found in the other file that deals with our method

#generate_data is the function that generated the data with sample size n, number of variables p (p=m from the manuscript) and structure='VAR' or 'GARCH' and heavy_tailed=TRUE or FALSE. 
#This function is literally just a translation of the method from R using CHATGPT. 
#For generating random graph we use the function nx.erdos_renyi_graph(p, 1/p, directed=True) from igraph library
#Lines 200-300 are the main part. We generate data with random graph + estimates the graph using one of the methods + compute the distance between true graph and estimated graph + repeat 100 times and return the mean of the distances
#We maually rewrote the values for each combination of 'structure' and 'heavy_tailed' into excel (easier since we dealt with python + R results)
#The table with final results can be found within the R code file

#If any issue is spotted, contact juraj.bodik@unil.ch. Python is not my preffered language and many lines are just chatGPT translated from R
import numpy as np
import igraph as ig
import random
import matplotlib.pyplot as plt
import networkx as nx


import pandas as pd
import matplotlib
from matplotlib import pyplot as plt
import sklearn
from scipy.stats import pareto, cauchy, norm


import tigramite as tg
from tigramite.independence_tests.robust_parcorr import RobustParCorr
from tigramite.pcmci import PCMCI

import tigramite
from tigramite import data_processing as pp
from tigramite.toymodels import structural_causal_processes as toys

from tigramite import plotting as tp
from tigramite.pcmci import PCMCI
from tigramite.lpcmci import LPCMCI

from tigramite.independence_tests.parcorr import ParCorr
from tigramite.independence_tests.robust_parcorr import RobustParCorr
from tigramite.independence_tests.parcorr_wls import ParCorrWLS
from tigramite.independence_tests.gpdc import GPDC
from tigramite.independence_tests.cmiknn import CMIknn
from tigramite.independence_tests.cmisymb import CMIsymb
from tigramite.independence_tests.gsquared import Gsquared
from tigramite.independence_tests.regressionCI import RegressionCI







def generate_data(n=200, p=4, structure='VAR', heavy_tailed=True):
   
    def generate_data_with_possible_NA(n, p, structure, heavy_tailed): #there is a small chance that if there are too many arrows pointing into one vertex, it will explode since the stationarity will be violated. I was lazy and if this happens we just repeat while this is not the case
        x = pd.DataFrame(np.zeros((n, p)))
        epsilon = pd.DataFrame(np.zeros((n, p)))
        true_graph = nx.erdos_renyi_graph(p, 1/p, directed=True)
       
        if not heavy_tailed:
            for i in range(p):
                epsilon.iloc[:, i] = norm.rvs(size=n)
        else:
            if structure == 'VAR':
                for i in range(p):
                    epsilon.iloc[:, i] = pareto.rvs(1, size=n)
            elif structure == 'GARCH':
                for i in range(p):
                    epsilon.iloc[:, i] = cauchy.rvs(size=n)
       
        if structure == 'VAR':
            effect = 0.3
            for j in range(2, n):
                for i in range(p):
                    addition = sum((true_graph.has_edge(k, i) * effect * x.iloc[j-1, k]) for k in range(p))
                    x.iloc[j, i] = 0.3 * x.iloc[j-1, i] + addition + epsilon.iloc[j, i]
       
        elif structure == 'GARCH':
            effect = 0.5 if not heavy_tailed else 5
            for j in range(2, n):
                for i in range(p):
                    addition = sum((true_graph.has_edge(k, i) * effect * x.iloc[j-np.random.choice(range(1,2)), k]**2) for k in range(p))
                    x.iloc[j, i] = (0.1 + addition)**0.5 * epsilon.iloc[j, i]
       
        return {'data': x, 'true_graph': true_graph}
   
    x = generate_data_with_possible_NA(n, p, structure, heavy_tailed)
    while x['data'].iloc[n-1].isna().sum() != 0:
        x = generate_data_with_possible_NA(n, p, structure, heavy_tailed)
   
    return {'data': x['data'], 'true_graph': x['true_graph']}






def get_edges_from_graph(graph):
    max_node = max(graph.keys()) + 1
    adj_matrix = [[0] * max_node for _ in range(max_node)]

    # Fill the adjacency matrix based on the edges in the graph
    for node, edges in graph.items():
        for edge in edges:
            dest_node = edge[0]
            if node != dest_node:  # Exclude loop edges
                adj_matrix[node][dest_node] = 1

    edges = []
    # Iterate over the adjacency matrix to find edges
    for i in range(len(adj_matrix)):
        for j in range(len(adj_matrix[i])):
            if adj_matrix[i][j] == 1:
                edges.append((j, i))

    return edges


def distance_between_two_graphs(graph1, graph2):
    l = len(list(set(graph1.edges).intersection(graph2.edges)))
    return len(graph1.edges) + len(graph2.edges) - 2 * l


# Define the function for plotting two directed graphs side by side
def plot_directed_graphs_side_by_side(graph1, graph2):
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    nx.draw(graph1, ax=axes[0], with_labels=True, node_color='lightblue', node_size=500, arrows=True)
    axes[0].set_title('Graph 1')

    nx.draw(graph2, ax=axes[1], with_labels=True, node_color='lightgreen', node_size=500, arrows=True)
    axes[1].set_title('Graph 2')

    plt.tight_layout()
    plt.show()







def PCMCI_RobustParCorr_one_simulation(n=1000, p=4,  structure = 'VAR',  heavy_tailed = False):
    data = generate_data(n, p, structure, heavy_tailed)
   
    true_graph = data['true_graph']
    column_names = [f'X{i}' for i in range(1, data['data'].shape[1] + 1)]
    data = pd.DataFrame(data['data'].values, columns=column_names)
   
    tau_max = 1
    pcmci = PCMCI(dataframe=pp.DataFrame(data.values), cond_ind_test=RobustParCorr())
    results = pcmci.run_pcmci(tau_max=tau_max, tau_min=1, pc_alpha=None)
    graph = pcmci.all_parents
   
    estimated = nx.DiGraph(get_edges_from_graph(graph))
    true_graph = nx.DiGraph(list(true_graph.edges))
   
    return distance_between_two_graphs(estimated, true_graph)


def PCMCI_GPDC_one_simulation(n=1000, p=4,  structure = 'VAR',  heavy_tailed = False):
    data = generate_data(n, p, structure, heavy_tailed)
   
    true_graph = data['true_graph']
    column_names = [f'X{i}' for i in range(1, data['data'].shape[1] + 1)]
    data = pd.DataFrame(data['data'].values, columns=column_names)
   
    tau_max = 1
    pcmci = PCMCI(dataframe=pp.DataFrame(data.values), cond_ind_test=GPDC())
    results = pcmci.run_pcmci(tau_max=tau_max, tau_min=1, pc_alpha=None)
    graph = pcmci.all_parents
   
    estimated = nx.DiGraph(get_edges_from_graph(graph))
    true_graph = nx.DiGraph(list(true_graph.edges))
   
    return distance_between_two_graphs(estimated, true_graph)
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


structure = 'VAR'
heavy_tailed = True

# Define the ranges for n and p
n_values = [500, 5000]
p_values = [2,4,7, 10, 20]
number_of_repetitions = 100
# Initialize a list to store results
results = []

# Loop over all combinations of n and p
for n in n_values:
    for p in p_values:
        average_error = 0
        for _ in range(number_of_repetitions):
            average_error += PCMCI_RobustParCorr_one_simulation(n=n, p=p, structure = structure, heavy_tailed=heavy_tailed)
        results.append({'n': n, 'p': p, 'average_error': average_error/number_of_repetitions})
# Convert results to a DataFrame
results = pd.DataFrame(results)
results






structure = 'VAR'
heavy_tailed = True

# Define the ranges for n and p
n_values = [500]
p_values = [2,4,7]
number_of_repetitions = 100
# Initialize a list to store results
results = []

# Loop over all combinations of n and p
for n in n_values:
    for p in p_values:
        average_error = 0
        for _ in range(number_of_repetitions):
            average_error += PCMCI_GPDC_one_simulation(n=n, p=p, structure = structure, heavy_tailed=heavy_tailed)
        results.append({'n': n, 'p': p, 'average_error': average_error/number_of_repetitions})
# Convert results to a DataFrame
results = pd.DataFrame(results)
results












